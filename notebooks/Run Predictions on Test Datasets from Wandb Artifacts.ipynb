{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import conllu\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import ast\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import json\n",
    "import shutil\n",
    "run = wandb.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porttinari_test_path = Path(\"../data/UD_Portuguese-Porttinari/pt_porttinari-ud-test.conllu\")\n",
    "dante_test_path = Path(\"../data/UD_Portuguese-DANTE/pt_dante-ud-test.conllu\")\n",
    "petrogold_test_path = Path(\"../data/UD_Portuguese-PetroGold/pt_petrogold-ud-test.conllu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porttinari_models = [\n",
    "    \"model-2s49nst7:v0\", # 42\n",
    "    \"model-tc1zzw4k:v0\", # 43\n",
    "    \"model-bm6gp43m:v0\", # 44\n",
    "    \"model-1o0gqxr3:v0\", # 45\n",
    "    \"model-31361v0k:v0\", # 46\n",
    "    \"model-3rjlz0lt:v0\", # 47\n",
    "    \"model-2bw1espl:v0\", # 48\n",
    "    \"model-v70g08fj:v0\", # 49\n",
    "    \"model-9ntjk1ug:v0\", # 50\n",
    "    \"model-kymqot4e:v0\", # 51\n",
    "]\n",
    "dante_models = [\n",
    "    \"model-o0ojeh3x:v0\", # 42\n",
    "    \"model-hfcre0cn:v0\", # 43\n",
    "    \"model-sm3hnlkk:v0\", # 44\n",
    "    \"model-tv4yte30:v0\", # 45\n",
    "    \"model-66omv4vd:v0\", # 46\n",
    "    \"model-2l1q1co8:v0\", # 47\n",
    "    \"model-19fciu72:v0\", # 48\n",
    "    \"model-2akpbr05:v0\", # 49\n",
    "    \"model-1b7kowrx:v0\", # 50\n",
    "    \"model-2vl5c8as:v0\", # 51\n",
    "]\n",
    "petrogold_models = [\n",
    "    \"model-1q90myqb:v0\", # 42\n",
    "    \"model-2v9qux4u:v0\", # 43\n",
    "    \"model-97qzwo1n:v0\", # 44\n",
    "    \"model-1yl1sxe4:v0\", # 45\n",
    "    \"model-2w6nmfoo:v0\", # 46\n",
    "    \"model-19jxdvv7:v0\", # 47\n",
    "    \"model-2k0a4cta:v0\", # 48\n",
    "    \"model-2gzdimee:v0\", # 49\n",
    "    \"model-23r6lkqz:v0\", # 50\n",
    "    \"model-1hbxjamt:v0\", # 51\n",
    "]\n",
    "porttinari_dante_models = [\n",
    "    \"model-16140tgi:v0\", # 42\n",
    "    \"model-1vj1vfst:v0\", # 43\n",
    "    \"model-1nw1r5gm:v0\", # 44\n",
    "    \"model-3ipj5wvo:v0\", # 45\n",
    "    \"model-2u66l64n:v0\", # 46\n",
    "    \"model-2t876ez4:v0\", # 47\n",
    "    \"model-3ftgmuns:v0\", # 48\n",
    "    \"model-26vtwt5o:v0\", # 49\n",
    "    \"model-13sziv1x:v0\", # 50\n",
    "    \"model-3s1i1gig:v0\", # 51\n",
    "]\n",
    "porttinari_petrogold_models = [\n",
    "    \"model-3t2o4ugz:v0\", # 42\n",
    "    \"model-1jmh5yzi:v0\", # 43\n",
    "    \"model-25j07376:v0\", # 44\n",
    "    \"model-3ub7r5rd:v0\", # 45\n",
    "    \"model-vzsfwq56:v0\", # 46\n",
    "    \"model-29r6wb4o:v0\", # 47\n",
    "    \"model-39kwlzfy:v0\", # 48\n",
    "    \"model-37a43ty6:v0\", # 49\n",
    "    \"model-cbw7mob2:v0\", # 50\n",
    "    \"model-1m7fe4k1:v0\", # 51\n",
    "]\n",
    "dante_petrogold_models = [\n",
    "    \"model-3lesa1b2:v0\", # 42\n",
    "    \"model-vlmx70y6:v0\", # 43\n",
    "    \"model-2kyu5jev:v0\", # 44\n",
    "    \"model-216u2b9d:v0\", # 45\n",
    "    \"model-bnm5t8fw:v0\", # 46\n",
    "    \"model-25rt674h:v0\", # 47\n",
    "    \"model-1vwxg7v2:v0\", # 48\n",
    "    \"model-3h6mbbp2:v0\", # 49\n",
    "    \"model-suxb490l:v0\", # 50\n",
    "    \"model-3evjf8e7:v0\", # 51\n",
    "]\n",
    "porttinari_dante_petrogold_models = [\n",
    "    \"model-2pf394eb:v0\", # 42\n",
    "    \"model-2c5f8s18:v0\", # 43\n",
    "    \"model-2oxpwyg9:v0\", # 44\n",
    "    \"model-a4v60z6o:v0\", # 45\n",
    "    \"model-35wuwdty:v0\", # 46\n",
    "    \"model-3nr9mytc:v0\", # 47\n",
    "    \"model-3dnmzhw0:v0\", # 48\n",
    "    \"model-azc3woza:v0\", # 49\n",
    "    \"model-n7rgstlo:v0\", # 50\n",
    "    \"model-1p59bqxh:v0\", # 51\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in porttinari_dante_petrogold_models:\n",
    "    artifact = run.use_artifact(f\"huber-ai/pos_porttinari_dante_petrogold/{model_name}\", type=\"model\")\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(artifact_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "\n",
    "    for test_file in [porttinari_test_path, dante_test_path, petrogold_test_path]:\n",
    "        out_path = f\"../tmp/{model_name.replace(':', '-')}\"\n",
    "        Path(out_path).mkdir(exist_ok=True)\n",
    "        \n",
    "        parent_dir = test_file.parent\n",
    "        data = {\n",
    "            \"id\": [],\n",
    "            \"tokens\": [],\n",
    "            \"tags\": [],\n",
    "        }\n",
    "\n",
    "        with open(str(test_file), \"r\", encoding=\"utf-8\") as in_f:\n",
    "            set_name = re.findall(r\"-(train|dev|test)\\.conllu\", str(test_file))[0]\n",
    "            dataset_name = test_file.stem\n",
    "            gold_sents = conllu.parse(in_f.read())\n",
    "            \n",
    "            for sent in gold_sents:\n",
    "                token_list = []\n",
    "                tag_list = []\n",
    "                for token in sent:\n",
    "                    if isinstance(token[\"id\"], int):\n",
    "                        token_list.append(token[\"form\"])\n",
    "                        tag_list.append(token[\"upos\"])\n",
    "\n",
    "                data[\"id\"].append(sent.metadata[\"sent_id\"])\n",
    "                data[\"tokens\"].append(token_list)\n",
    "                data[\"tags\"].append(tag_list)\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            processed_filename = parent_dir.joinpath(f\"{set_name}.csv\")\n",
    "            df.to_csv(processed_filename, index=False)\n",
    "        \n",
    "        dataset = datasets.load_dataset(\"csv\", data_files={set_name: str(processed_filename)})\n",
    "\n",
    "        def str_to_list(example):\n",
    "            example[\"tokens\"] = ast.literal_eval(example[\"tokens\"])\n",
    "            example[\"tags\"] = ast.literal_eval(example[\"tags\"])\n",
    "            return example\n",
    "        \n",
    "        for set_name, dataset_sub in dataset.items():\n",
    "            dataset[set_name] = dataset_sub.map(str_to_list)\n",
    "        \n",
    "        def tokenize_and_align_labels(examples, label_all_tokens = False):\n",
    "            for i, sample in enumerate(examples[\"tokens\"]):\n",
    "                examples[\"tokens\"][i] = [tk.replace(\"\", \"*\").replace(\"\", \"*\") for tk in sample]\n",
    "            # print(examples[\"tokens\"], examples[\"tokens\"][0])\n",
    "            # tokens = [tk.replace(\"\", \"*\") for tk in examples[\"tokens\"]]\n",
    "            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[\"tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:\n",
    "                    # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                    # ignored in the loss function.\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    # We set the label for the first token of each word.\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        label_ids.append(model.config.label2id.get(label[word_idx], model.config.label2id[\"X\"]))\n",
    "                    # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                    # the label_all_tokens flag.\n",
    "                    else:\n",
    "                        label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                    previous_word_idx = word_idx\n",
    "\n",
    "                labels.append(label_ids)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "        outputs = {}\n",
    "        for sample in tqdm(tokenized_dataset[\"test\"], desc=\"Running model...\"):\n",
    "\n",
    "            output = model(torch.tensor([sample[\"input_ids\"]]))\n",
    "            outputs[sample[\"id\"]] = output\n",
    "\n",
    "        \n",
    "        for sample in tqdm(tokenized_dataset[set_name], desc=\"Processing model outputs...\"):\n",
    "            output = outputs[sample[\"id\"]]\n",
    "\n",
    "            i_token = 0\n",
    "            labels = []\n",
    "            scores = []\n",
    "            # print(sample[\"labels\"], sample.keys())\n",
    "            assert len(sample[\"labels\"]) == len(output.logits[0]), \"Sentence {} contains {} true labels and {} predictions\".format(sample[\"id\"], len(sample[\"labels\"]), len(output.logits[0]))\n",
    "            for original_label, pred in zip(\n",
    "                sample[\"labels\"],\n",
    "                output.logits[0],\n",
    "            ):\n",
    "                if original_label == -100:\n",
    "                    continue\n",
    "                label = model.config.__dict__[\"id2label\"][int(pred.argmax(axis=-1))]\n",
    "                labels.append(label)\n",
    "                scores.append(\n",
    "                    \"{:.2f}\".format(100 * float(torch.softmax(pred, dim=-1).detach().max()))\n",
    "                )\n",
    "                i_token += 1\n",
    "            assert i_token == len(sample[\"tokens\"]), \"Sentence {} produced {} tokens, but it has {} tokens\".format(sample[\"id\"], i_token, len(sample[\"tokens\"]))\n",
    "            output[\"predicted_labels\"] = labels\n",
    "            output[\"predicted_scores\"] = scores\n",
    "        \n",
    "        pred_sents = conllu.parse(open(str(test_file), \"r\", encoding=\"utf-8\").read())\n",
    "        for sent in tqdm(pred_sents, desc=f\"Creating output file for {dataset_name}\"):\n",
    "            output = outputs[sent.metadata[\"sent_id\"]]\n",
    "            i = 0\n",
    "            for token in sent:\n",
    "                if isinstance(token[\"id\"], int):\n",
    "                    # print(output[\"predicted_labels\"], sent.metadata[\"sent_id\"], token[\"form\"], i)\n",
    "                    token[\"upos\"] = output[\"predicted_labels\"][i]\n",
    "                    i += 1\n",
    "        pred_path = test_file.parent.joinpath(f\"{dataset_name}_pred.conllu\")\n",
    "        with open(pred_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            out_f.writelines([sentence.serialize() + \"\\n\" for sentence in pred_sents])\n",
    "        \n",
    "        def get_tags(sents, min_tokens = 0, max_tokens = 60):\n",
    "            true_tags = []\n",
    "            for sent in sents:\n",
    "                tags = []\n",
    "                n_tokens = 0\n",
    "                for token in sent:\n",
    "                    if isinstance(token[\"id\"], int):\n",
    "                        tags.append(token[\"upos\"])\n",
    "                        n_tokens += 1\n",
    "                if n_tokens >= min_tokens and n_tokens <= max_tokens:\n",
    "                    true_tags += tags\n",
    "            return true_tags\n",
    "\n",
    "        true_tags = get_tags(gold_sents)\n",
    "        pred_tags = get_tags(pred_sents)\n",
    "\n",
    "        for i in range(len(pred_tags)):\n",
    "            if pred_tags[i] == \"_\":\n",
    "                pred_tags[i] = \"X\"\n",
    "\n",
    "        cls_report = classification_report(true_tags, pred_tags)\n",
    "        cls_report_path = test_file.parent.joinpath(f\"{dataset_name}_cls_report.txt\")\n",
    "        with open(cls_report_path, \"w\") as out_f:\n",
    "            out_f.writelines(cls_report)\n",
    "\n",
    "        acc = accuracy_score(true_tags, pred_tags)\n",
    "        f1_macro = f1_score(true_tags, pred_tags, average=\"macro\")\n",
    "        f1_weighted = f1_score(true_tags, pred_tags, average=\"weighted\")\n",
    "\n",
    "        json_path = test_file.parent.joinpath(f\"{dataset_name}_results.json\")\n",
    "        with open(json_path, \"w\") as out_f:\n",
    "            json.dump({\n",
    "                f\"{dataset_name}_acc\": acc,\n",
    "                f\"{dataset_name}_f1_macro\": f1_macro,\n",
    "                f\"{dataset_name}_f1_weighted\": f1_macro,\n",
    "            }, out_f)\n",
    "\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(true_tags)\n",
    "        true_list = list(map(lbl.transform, [true_tags]))\n",
    "        pred_list = list(map(lbl.transform, [pred_tags]))\n",
    "\n",
    "        cm = confusion_matrix(true_list[0], pred_list[0])\n",
    "\n",
    "        df_cm = pd.DataFrame(\n",
    "            cm, index=lbl.classes_, columns=lbl.classes_,\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(17,17))\n",
    "        matplotlib.rcParams['figure.dpi'] = 300\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, cmap=\"gray\", fmt=\"d\", annot_kws={\"size\": 14})\n",
    "        heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "        heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "        cf_matrix_path = test_file.parent.joinpath(f\"{dataset_name}_cf_matrix.png\")\n",
    "        plt.savefig(cf_matrix_path)\n",
    "\n",
    "        for artifact_file in [cf_matrix_path, json_path, cls_report_path, pred_path, test_file]:\n",
    "            shutil.copy(str(artifact_file), out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d98ec90a84bbb2de45e534f5574e1318b8536406b7ead65fd4c605a6cfa9ec34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
